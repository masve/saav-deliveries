{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date\n",
    "from dateutil.rrule import rrule, DAILY\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "from __future__ import division\n",
    "import geoplotlib as glp\n",
    "from geoplotlib.utils import BoundingBox, DataAccessObject\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import itertools\n",
    "from sklearn import cluster\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Traffic and Weather - Explainer Notebook\n",
    "*This notebook supports project visualization site found [here](https://masve.github.io/saav-deliveries) for DTU course [Social data analysis and visualization (02806)](http://www.kurser.dtu.dk/2013-2014/02806.aspx?menulanguage=en-GB)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "> What is your dataset?\n",
    "\n",
    "Our dataset is [NYPD Motor Vehicle Collisions](https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions/h9gi-nx95). It contains records for every reported incident in the NYC area. Records are available from July 2012 till today. Specifically, where the collision took place, the cause of the collision, injuries, fatalities and more.\n",
    "\n",
    "The other dataset used was an extraction of weather conditions for the NYC area. This was pulled from [Weather Underground](https://www.wunderground.com) using their csv service for hourly updates. For each hour, there are measurements of the temperature, visibility, windspeed as well as the overall weather conditions such as Rain, Snow, Clear etc.\n",
    "\n",
    "> Why did you choose this/these particular dataset(s)?\n",
    "\n",
    "Choosing the collisions dataset, was mainly out of interest in finding out where, how and why collisions happen.\n",
    "\n",
    "The weather dataset was an afterthough and was mainly something we wanted to investigate after having looked collisions and the road conditions / traffic infrastructure.\n",
    "\n",
    "> What was your goal for the end user's experience?\n",
    "\n",
    "The main goal is to provide the user/reader with knowledge about how the weather affects the road and in that sense the collision rate. We would assume that more collisions happens in bad conditions, but can we quantify that? The user should also end up with an idea of where incidents happens the most, maybe to give an idea for the government of NYC for improvements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Stats. Let's understand the dataset better\n",
    ">Write about your choices in data cleaning and preprocessing\n",
    "\n",
    "Talking about the collisions dataset, the cleaning was mainly done for the columns that we knew we wanted to investigate or otherwise we though was important in the data exploration. Notably we wanted to make sure that we had location data for all the rows we investigated.\n",
    "\n",
    "The weather information was not directly available to us and required a lot of HTTP requests to Weather Undergrounds servers. Below is how we did it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#        Downloading Weather Data (KJFK)\n",
    "# ============================================\n",
    "\n",
    "# Getting weather data from wunderground\n",
    "start_date = date(2012, 7, 1)\n",
    "end_date = date(2016, 2, 29)\n",
    "\n",
    "# csv container for all daily weather infromation\n",
    "frames = []\n",
    "\n",
    "# url template for http requests. %s/%s/%s represent year/month/day\n",
    "url_template = 'https://www.wunderground.com/history/airport/KJFK/%s/%s/%s/DailyHistory.html?\\\n",
    "                req_city=New+York&req_state=NY&req_statename=New+York&reqdb.zip=10001\\\n",
    "                &reqdb.magic=4&reqdb.wmo=99999&format=1.csv'\n",
    "month = \"\"\n",
    "\n",
    "# Query wunderground for daily weather information (returned as csv)\n",
    "for dt in rrule(DAILY, dtstart=start_date, until=end_date):\n",
    "    if (month != dt.strftime(\"%m\")):\n",
    "        month = dt.strftime(\"%m\")\n",
    "        print 'Downloading to memory: ' + dt.strftime(\"%Y-%m\")\n",
    "    # download and append csv file to csv container\n",
    "    frames.append(pd.read_csv(url_template % (dt.strftime(\"%Y\"),dt.strftime(\"%m\"), dt.strftime(\"%d\"))))\n",
    "\n",
    "# Combine all csv's to one big and save it\n",
    "print \"Saving data to csv...\"\n",
    "data = pd.concat(frames)\n",
    "data.to_csv('weather_data_nyc_kjfk.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two datasets. Collisions and weather. However to avoid having to lookup in in a secondary dataset, that is the weather information, we merged the two datasets together. For the most part we had weather data for each hour for all the rows we wanted to investigate, with only a combined gap of a couple of days. In additions some hours had more than one row of weather information. We ignored these factors as we saw them insignificant to the overall result anyways.\n",
    "\n",
    "In order to join the datasets they both had to have some columns in common. Which needed to be the date and time (hour). The weather dataset already had a datetime column in UTC. What we did was convert to NYC local time and add the columns Year, Month, Day, and Hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#         Cleaning the Weather Data\n",
    "# ============================================\n",
    "\n",
    "# Read downloaded dataset\n",
    "weather = pd.read_csv('datasets/weather_data_nyc_kjfk.csv')\n",
    "\n",
    "# Convert UTC time to NYC actual\n",
    "def UTCtoActual(utcDate):\n",
    "    from_zone = tz.gettz('UTC')\n",
    "    to_zone = tz.gettz('America/New_York')\n",
    "    \n",
    "    utc = datetime.strptime(utcDate.DateUTC, '%Y-%m-%d %H:%M:%S')\\\n",
    "                  .replace(tzinfo=from_zone)\\\n",
    "                  .astimezone(to_zone)\n",
    "    s = pd.Series([utc.year, utc.month, utc.day, utc.hour])\n",
    "    s.columns = ['Year', 'Month', 'Day', 'Hour']\n",
    "    return s\n",
    "\n",
    "# Apply the above function to every row in the weather dataset and save the file.\n",
    "weather[['Year', 'Month', 'Day', 'Hour']] = weather.apply(UTCtoActual, axis=1)\n",
    "weather.to_csv('datasets/weather_data_nyc_kjfk_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both datasets now having a 'common' ground for joining. This can now be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#              Merging Datasets\n",
    "#        NYPD Motor Vehicle Collisions\n",
    "#                    and \n",
    "#      Weather Underground Extract (KJFK)\n",
    "# ============================================\n",
    "\n",
    "# Read the datasets to be merged\n",
    "incidents = pd.read_csv('datasets/NYPD_Motor_Vehicle_Collisions.csv')\n",
    "weather = pd.read_csv('datasets/weather_data_nyc_kjfk_clean2.csv')\n",
    "\n",
    "# Features from the dataset to merge in\n",
    "features = ['Conditions', 'Precipitationmm', \\\n",
    "            'TemperatureC', 'VisibilityKm']\n",
    "\n",
    "# Looks up weather data on the date and hour requested\n",
    "def lookup_weather2(year, month, day, hour):\n",
    "    w = weather[(weather.Year == year) & (weather.Month == month) & (weather.Day == day) & (weather.Hour == hour)]\n",
    "    return w\n",
    "\n",
    "# Looks up weather data, if nothing is found on the hour, it either looks an hour ahead or back.\n",
    "# Otherwise returns empty.\n",
    "def lookup_weather(date, time):\n",
    "    month = int(date.split('/')[0])\n",
    "    day = int(date.split('/')[1])\n",
    "    year = int(date.split('/')[2])\n",
    "    hour = int(time.split(':')[0])\n",
    "    d = lookup_weather2(year, month, day, hour).head(1)\n",
    "    if (d.empty):\n",
    "        dt_back = datetime.datetime(year, month, day, hour) - datetime.timedelta(hours=1)\n",
    "        dt_forward = datetime.datetime(year, month, day, hour) + datetime.timedelta(hours=1)\n",
    "        \n",
    "        d_back = lookup_weather2(dt_back.year, dt_back.month, dt_back.day, dt_back.hour)\n",
    "        if (not d_back.empty): return d_back\n",
    "        \n",
    "        d_forward = lookup_weather2(dt_forward.year, dt_forward.month, dt_forward.day, dt_forward.hour)\n",
    "        if (not d_forward.empty): return d_forward\n",
    "    return d\n",
    "\n",
    "# Merges the datasets\n",
    "def merge_weather(incident):\n",
    "    date = incident.DATE\n",
    "    time = incident.TIME\n",
    "\n",
    "    w = lookup_weather(date, time)\n",
    "\n",
    "    try:\n",
    "        # Default values\n",
    "        con = \"-\"\n",
    "        temp = \"-\"\n",
    "        rainmm = \"-\"\n",
    "        viskm = \"-\"\n",
    "\n",
    "        # If weather data is different from null\n",
    "        if (not pd.isnull(w['Conditions'].iloc[0])):\n",
    "            con = w['Conditions'].iloc[0]\n",
    "        if (not pd.isnull(w['TemperatureC'].iloc[0])):\n",
    "            temp = w['TemperatureC'].iloc[0]\n",
    "        if (not pd.isnull(w['Precipitationmm'].iloc[0])):\n",
    "            rainmm = w['Precipitationmm'].iloc[0]\n",
    "        if (not pd.isnull(w['VisibilityKm'].iloc[0])):\n",
    "            viskm = w['VisibilityKm'].iloc[0]\n",
    "            \n",
    "        s = pd.Series([con, rainmm, temp, viskm])\n",
    "        return s\n",
    "    except:\n",
    "        print date + \"x\" + time\n",
    "        s = pd.Series([None,None,None,None])\n",
    "        return s\n",
    "    \n",
    "print \"Applying weather data to incidents...\"\n",
    "incidents[features] = incidents[incidents.DATE.str.split('/').str.get(2) != '2016'].apply(merge_weather, axis=1)\n",
    "print \"Saving weather in-riched incident data...\"\n",
    "incidents.to_csv('datasets/NYPD_Motor_Vehicle_Collisions_Weather_FINAL.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was how we preprocessed and got all the available information that we require for making our data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)\n",
    "\n",
    "Our primary focus is on the dataset addressing NYC Motor Vehicle Collisions and the merged dataset, consisting of incidents along with their weather conditions.\n",
    "\n",
    "#### NYPD Motor Vehicle Collisions\n",
    "\n",
    "[Source](https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions/h9gi-nx95)\n",
    "\n",
    "The dataset contained within a csv file is of size 149MB, with 29 features and 769054 records.\n",
    "\n",
    "<img src=\"explainer_pictures/nyc_motor_features_2.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "We started on getting finding answers to our burning questions and some basic statistics on our data. Some questions that we wanted to address was the number of collisions that are happening within each borough. As an example shown in below chart, it shows the number of incidents recorded within each borough over the years.\n",
    "\n",
    "<img src=\"explainer_pictures/collisioncount_by_borough_year.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "Further investigation on the dataset, we made use of the geolocation to get an even more precise geographical representation on where incidents are happening than just grouping by borough. We presented the location of incidents in a heatmap by using geoplotlib.kde to map and show the density incidents in their respective geographical location. \n",
    "\n",
    "<img src=\"explainer_pictures/clustering_driverinattention_distraction.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "#### Weather Underground\n",
    "\n",
    "[Source](https://www.wunderground.com), processed dataset can he found [here](https://github.com/masve/saav-deliveries/blob/master/project/datasets/weather_data_nyc_kjfk_clean2.csv.zip).\n",
    "\n",
    "The dataset for the weather, is contained within a csv file with a file size of 3.9MB. The data is constructed with 22 features and 36002 rows.\n",
    "\n",
    "Now that we have made some modelling on the collision data, we wanted to know the cause of action for these collisions. There are definitely many contribufaction factors for each recorded incidents, however we thought there could be other factors that lies beneath these incidents, which is why we chose a common external factor, the weather. \n",
    "\n",
    "We retrieved a set of weather data, from the same time period as the dataset for the motor vehicle collisions and merged them by the time. The data contains 30 different weather conditions\n",
    "\n",
    "<table align=\"center\">\n",
    "    <tr>\n",
    "      <th>\n",
    "          <img src=\"explainer_pictures/weatherdata_conditions_1.png\" style=\"width: 180px;\"><br>\n",
    "      </th>\n",
    "      <th>\n",
    "          <img src=\"explainer_pictures/weatherdata_conditions_2.png\" style=\"width: 180px;\"><br>\n",
    "      </th>\n",
    "    </tr>  \n",
    "</table>\n",
    "\n",
    "#### Motor Vehicle Collisions and Weather data\n",
    "\n",
    "After a combination of the two dataset, the csv file size is 175.1MB, with 34 features and 769054 rows. The merged dataset can be found [here](https://github.com/masve/saav-deliveries/blob/master/project/datasets/NYPD_Motor_Vehicle_Collisions_weather4.7z)\n",
    "\n",
    "With the merged dataset, we now have the weather conditions for each recorded collisions and we can get the frequency of collision per hour for a specific weather condition. The normalized collisions frequency, we get the ratio of collisions for each weather condition with a reference to Mostly Cloudy, as this weather conditions are the one having the most recorded incidents. The collisitons frequency is shown below generated in Python. A beatufied visualisation can be shown on the [website](https://masve.github.io/saav-deliveries/).\n",
    "\n",
    "<img src=\"explainer_pictures/weatherdata_chart_conditions.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory. Which theoretical tools did you use?\n",
    "\n",
    "> Describe which machine learning tools you use and why the tools you've chosen are right for the problem you're solving.\n",
    "\n",
    "Our motivation for investigating the collisions dataset was mainly to see if we would be able to find out where, how and why collisions happen in NYC. Could we predict where accidents are likely to happen? In class we have discussed two methods for classifiers that could be used to predict where the next crime was likely to happen. That is, KNN and Decision Trees.\n",
    "\n",
    "In the end we choose to continue with the decision tree approach for classification. This was mainly due to the features in our dataset speciafically the weather features that we ended up working with could easily be categorized. In addition, for us atleast, we could easily prevent overfitting and do cross-validation (see below).\n",
    "\n",
    "In addition we used K-Means for clustering all incidents by location which data was used to power the decision tree classifier. In other words we used K-means in order to categorize the locational data (latitude and longitude) into clusters.  More on this later.\n",
    "\n",
    "> Talk about your model selection. How did you split the data in to test/training. Did you use cross validation?\n",
    "\n",
    "With around 650,000 rows to be split between test and training we were not really concered with too high variance. The split as documented below was 80/20.\n",
    "\n",
    "From the get go we know that finding the exact intersection where a collision is predictively going to take place is unfeasible to say the least. Doing some initial exploration using a trained decision tree targetting a longitude, latitude tuple we found a prediction score of 4-5%. \n",
    "\n",
    "If we cannot with great accuracy look at individual intersections we could look at zip codes. We could ask the question; in which postal area should we put an ambulance, given the weather and hour of day? Still the accuracy is  low. One way to improve the accuracy would be to add additional non-correlated features. Using K-Means to make location group-based clustering, that is longitude and latitude, how does that improve the model?\n",
    "\n",
    "As mentioned we are using of using sklearns [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) we ended up using the [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). We will be starting out with training a random forest of 50 trees. This should prevent any overfitting. In addition we use cross-validation to also test the performance of the model. See below.\n",
    "\n",
    "**In summary we are training a random forest of 50 trees, targetting/predicting/classifying ```ZIP CODE```, by using the features, ```KMEANS (location-based-clusters)```, ```HOUR```, ```Condition```.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMEANS LOCATION CLUSTERS CALCULATED\n",
      "TRAINED FOREST WITH 467746 SAMPLES\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "#           Train Random Forest\n",
    "#\n",
    "# 1. Preprocess dataset for training\n",
    "# 2. Add Kmean clustering to dataset\n",
    "# 3. Split dataset into training and test\n",
    "# 4. Train forest with dataset\n",
    "# ============================================\n",
    "\n",
    "# Encodes the string values in the dataframe to integers. (Classifier does not support string values)\n",
    "def encode_column(df, target_column):\n",
    "    df_mod = df.copy()\n",
    "    targets = pd.Series(df_mod[target_column].unique())\n",
    "    map_to_int = {name: n for n, name in enumerate(targets)}\n",
    "    df_mod[target_column+\"_encoded\"] = df_mod[target_column].replace(map_to_int)\n",
    "    return (df_mod, targets)\n",
    "\n",
    "# Trains the forest (50 trees) on a dataset, with a target and features\n",
    "def train_tree(target, features, dataset):\n",
    "    clf = RandomForestClassifier(n_estimators=50, n_jobs=2)\n",
    "    X = np.array(dataset[features])\n",
    "    Y = np.array(list(itertools.chain(*dataset[[target]].values)))\n",
    "    return clf.fit(X, Y)\n",
    "\n",
    "# Calculates kmean clusters and cluster idenfiers and centroids \n",
    "def kmeans(k, dataset, colums):\n",
    "    md = cluster.KMeans(n_clusters=k).fit(dataset[colums])\n",
    "    return md.predict(dataset[colums]),md.cluster_centers_\n",
    "\n",
    "# Load, filter and seperate time\n",
    "data = pd.read_csv('datasets/NYPD_Motor_Vehicle_Collisions_weather4.csv')\n",
    "data = data[pd.notnull(data['LOCATION'])]\n",
    "data['HOUR'] = data.TIME.str.split(':').str.get(0)\n",
    "\n",
    "# Label that is target for prediction, filter dataset for target label\n",
    "target_label = 'ZIP CODE'\n",
    "data = data[pd.notnull(data[target_label])]\n",
    "\n",
    "# Encode target/label column\n",
    "mdata, target = encode_column(data, target_label)\n",
    "\n",
    "# Encode the feature columns\n",
    "mdata, _ = encode_column(mdata, 'Conditions')\n",
    "\n",
    "# Kmean location clusters (30 clusters calculated)\n",
    "s, cen = kmeans(30, mdata, ['LONGITUDE', 'LATITUDE'])\n",
    "mdata['KMEANS'] = s\n",
    "print \"KMEANS LOCATION CLUSTERS CALCULATED\"\n",
    "\n",
    "# Features for prediction\n",
    "features = ['KMEANS','Conditions_encoded','HOUR']\n",
    "\n",
    "# Split data set into training and test data\n",
    "train_data = mdata.head(int(len(mdata.index) * 0.80))\n",
    "test_data = mdata.tail(int(len(mdata.index) * 0.20))\n",
    "\n",
    "target_label_encoded = target_label+'_encoded'\n",
    "\n",
    "# Train the random forest\n",
    "clf = train_tree(target_label_encoded, features, train_data)\n",
    "print \"TRAINED FOREST WITH %d SAMPLES\" % len(train_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Explain the model performance. How did you measure it? Are your results what you expected?\n",
    "\n",
    "The model performance was measured lettings the decision tree classify the test data. Each row in of itself already has the correct ZIP CODE. By letting the tree predict the ZIP CODE and comparing with the actual value we can test if the prediction is correct. Doing that over the entire set of test data we can find a percentage accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFIED 27735 SAMPLES CORRECTLY\n",
      "PREDICTION SCORE: 0.237181\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "#           Testing Random Forest\n",
    "# ============================================\n",
    "\n",
    "# Test the forest\n",
    "def test_forest(clf, test_data, features):\n",
    "    return clf.predict(test_data[features])\n",
    "\n",
    "# Tests test_data on the forest classifier clf with features and target_label.\n",
    "# encoded_map is a lookup table of the encoded values (numeric) to actual string values\n",
    "def test_prediction(target_label, clf, test_data, features, encoded_map):\n",
    "    corrects = 0\n",
    "    predictions = test_tree(clf, test_data[features], features)\n",
    "    for i in range(0, len(predictions)):\n",
    "        if predictions[i] == test_data.iloc[i][target_label]:\n",
    "            corrects += 1\n",
    "    print \"CLASSIFIED %d SAMPLES CORRECTLY\" % corrects\n",
    "    # Return model accuracy [%]\n",
    "    return corrects / len(predictions)\n",
    "\n",
    "# Test the decision tree\n",
    "print \"PREDICTION SCORE: %f\" % test_prediction(target_label_encoded, clf, test_data, features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score shows a prediction accuracy of ~23.7%. Based on the input features this result was we could not have predicted. It means that it is actually possible to predict the zip code in which a collosion will occour based on the hour of day, weather condition and improved by the location based clustering. To verify the result - even though we choose to use random forest because of it - a cross-validation on the model has been made.\n",
    "\n",
    "The cross-validation score are as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS-VALIDATION SCORE: 0.238919 (+/- 0.000835)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "#         Cross-validating the Model\n",
    "# ============================================\n",
    "scores = cross_val_score(clf, mdata[features].values, mdata[target_label_encoded].values, cv=5)\n",
    "print(\"CROSS-VALIDATION SCORE: %0.6f (+/- %0.6f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation score seem to confirm the accuracy of the model. It is as confident - with 5 passes - as the random forest classifier we setup previously.\n",
    "\n",
    "Lastly, the visualization of the decision tree can be found [here](https://github.com/masve/saav-deliveries/blob/master/project/datasets/tree.pdf). Note that this is for a single tree in the forest.\n",
    "\n",
    "And the K-means location cluster visualization can be found [here](https://github.com/masve/saav-deliveries/blob/master/project/datasets/kmeanscluster.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "> Explain the visualizations you've chosen.\n",
    "\n",
    "> Why are they right for the story you want to tell?\n",
    "\n",
    "The visualizations can be found on the site [here](https://masve.github.io/saav-deliveries). The visualizations are odered below, as they appear on the site.\n",
    "\n",
    "**Interactive Intersection Map**\n",
    "\n",
    "We wanted to get the reader a feel for how the scope of the data. Therefore we choose a this as the first visualization as it gives a good idea where accidents happen, mainly intersections, and it allows for the user to explore on their own getting a feel for their neighbourhood. To do this, we did not think a d3 visualization was sufficient. Therefore used an actual map - [service](http://mapbox.com/) built with the api [API](http://leafletjs.com/) -  and plotted out every collision on it. That meant that users could zoom, drag and click on each point to get additional information.\n",
    "\n",
    "**Top 10 Intersections by Collisions**\n",
    "\n",
    "This visualization was mainly a continuation of the map. It could be a little bit overwhelming, so we boiled it down to the 10 intersections where the most collisions occour, this again helped to setup the story we wanted to tell.\n",
    "\n",
    "**Frequency of Collisions in Weather Conditions**\n",
    "\n",
    "Now that we completed the introductory part of the story/analysis, we switch to the main part of the analysis. That was the collisions, is certain weather conditions. To start off, we wanted to find out if there was certain conditions that produced more collisions per hour than others. In some ways it did confirm our hypotheses and in others it left us wondering. The why is explained in on the site. Looking back, there are a lot of ways that we could have choosen to visualize this. We do not think we could have gotten around a bar chart but something like [this](http://www.bworldonline.com/DataViz/images/072415BigMac550.png) could be a variation that better shows the relative frequency from the baseline.\n",
    "\n",
    "**Distribution of Weather Conditions for the Cause: Pavement Slippery**\n",
    "\n",
    "At ths point in the story we are looking at weather-induced causes for collisions. In particular ```Pavement Slippery```. Again, this was a hypothesis. So we wanted to show if this was actually the case.\n",
    "\n",
    "**Top 10 Intersections with Cause: Pavement Slippery**\n",
    "\n",
    "Before looking into the predictive models we wanted to again show which intersections are the most dangerous when only looking at ```Pavement Slippery``` induced collisions. Could we possible find out why these accidents occour? To do so we plotted out the top 10 unique locations where this type of collisions occour the most. And what was found was that in intersections with curved/downhill roads leading to them are the worst. This helped us make a conclusion about why these type of collisions occour and which places that a reader should pay extra attention to.\n",
    "\n",
    "**Location-Based Clusters**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion. Think critically about your creation\n",
    "> What went well?,\n",
    "\n",
    "Generally we think that the [site](http://masve.github.io/saav-deliveries/) development and the visualizations we choose went well. Although we think we could have made a few variations in the bar charts shown, namely the Frequency of Collisions in Weather Conditions visualization as already mentioned.\n",
    "\n",
    "> What is still missing? What could be improved?, Why?\n",
    "\n",
    "A decision tree representation is missing. We think that it wouldbe cool to somehow have a visual representation in the like of what is seen on [this page](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/#test). However, we decided to not look too much into it as it seemed to be a very complex visualization.\n",
    "\n",
    "In addition the use of k-means clusters to help the random forest training, is in hindsight maybe not so great of an idea. Doing some very late validation, we found that a higher cluster could gave us better prediction accuracy. Although with deminishing returns, this could be coined to the fact that the decision tree 'learns' which cluster number a zip code is part of.\n",
    "\n",
    "Despite this, we do think that we have found some interesting points during the exploration of the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
